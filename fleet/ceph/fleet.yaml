defaultNamespace: rook-ceph

dependsOn:
  - name: aldaas-fleet-rook

helm:
  repo: https://charts.rook.io/release
  chart: rook-ceph-cluster
  values:
    cephClusterSpec:
      dataDirHostPath: /var/lib/rook
      continueUpgradeAfterChecksEvenIfNotHealthy: true
      mgr:
        count: 1
        modules:
          - name: pg_autoscaler
            enabled: true
      dashboard:
        enabled: true
        ssl: false
      placement:
        all:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: rook_cluster
                        operator: In
                        values:
                          - rook-ceph
                  topologyKey: kubernetes.io/hostname
      mon:
        count: 3
        allowMultiplePerNode: false
        volumeClaimTemplate:
          spec:
            storageClassName: global.fleet.clusterLabels.ceph-storage
            resources:
              requests:
                storage: 10Gi  
      storage:
        storageClassDeviceSets:
        - name: osd-pool
          count: 3
          portable: true
          encrypted: false
          volumeClaimTemplates:
          - metadata:
              name: data
            spec:
              resources:
                requests:
                  storage: 100Gi
              storageClassName: global.fleet.clusterLabels.ceph-storage
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
          placement:
            topologySpreadConstraints:
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
          preparePlacement:
            topologySpreadConstraints:
              - maxSkew: 1
                # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd-prepare
        onlyApplyOSDPlacement: false
    cephBlockPools:
      - name: replicated-metadata-pool
        spec:
          failureDomain: osd
          replicated:
            size: 3
          deviceClass: ssd
        storageClass:
          enabled: false
      - name: ec-data-pool
        spec:
          failureDomain: osd
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          deviceClass: ssd
        storageClass:
          enabled: true
          name: ceph-block
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          parameters:
            dataPool: ec-data-pool
            pool: replicated-metadata-pool
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
    cephFileSystems: []
    cephFileSystemVolumeSnapshotClass:
      enabled: false
    cephObjectStores: []
